<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[将Pandas中的DataFrame类型转换成Numpy中array类型的三种方法]]></title>
    <url>%2F2020%2F05%2F09%2F%E5%B0%86Pandas%E4%B8%AD%E7%9A%84DataFrame%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E6%88%90Numpy%E4%B8%ADarray%E7%B1%BB%E5%9E%8B%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/qq_30163461/article/details/80080529]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘大作业2记录]]></title>
    <url>%2F2020%2F05%2F07%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[数据挖掘-决策树任务一一、简介决策树摘自 scikitlearn中文文档 决策树（DT）是一种用于分类和回归的无参监督学习方法。其目的是创建一种模型从数据特征中学习简单的决策规则来预测一个目标变量的值。 建立决策树主要有三种算法： ID3 C4.5 CART(Classification And Regression Tree) ID3（Iterative Dichotomiser 3）由 Ross Quinlan 在1986年提出。该算法创建一个多路树，找到每个节点（即以贪心的方式）分类特征，这将产生分类目标的最大信息增益。决策树发展到其最大尺寸，然后通常利用剪枝来提高树对未知数据的泛化能力。 C4.5 是 ID3 的后继者，并且通过动态定义将连续属性值分割成一组离散间隔的离散属性（基于数字变量），消除了特征必须被明确分类的限制。C4.5 将训练的树（即，ID3算法的输出）转换成 if-then 规则的集合。然后评估每个规则的这些准确性，以确定应用它们的顺序。如果规则的准确性没有改变，则需要决策树的树枝来解决。 CART（Classification and Regression Trees （分类和回归树））与 C4.5 非常相似，但它不同之处在于它支持数值目标变量（回归），并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值来构造二叉树。 本次作业使用CART算法 决策树构建过程参考视频 https://www.bilibili.com/video/BV1T7411b7DG/ 交叉验证它的基本思想就是将原始数据（dataset）进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。 还可以从有限的数据中获取尽可能多的有效信息。 作业中采用10折交叉验证, 用DecisionTreeClassifier的参数剪枝，连续属性分裂值确定 参考 https://www.jianshu.com/p/40541aa440c7 ​ https://www.jb51.net/article/181619.htm ​ https://blog.csdn.net/wyp8268526/article/details/90273682 ​ https://blog.csdn.net/weixin_42969619/article/details/99302615 ​ https://blog.csdn.net/y0929/article/details/82686177 ​ https://www.bilibili.com/video/BV1m741187N6/ 二、软件环境准备Anaconda3 在Anaconda中安装sklearn包 conda install -c conda-forge sklearn Graphviz(需配置环境变量) 鸢尾花数据集(sklearn自带) 数据集介绍 数据量 150 属性数量 4 属性特征 实数（连续属性） 属性 萼片长度、萼片宽度、花瓣长度、花瓣宽度 类别数 3 类别 Iris Setosa、Iris Versicolor、Iris Virginica ​ Iris 鸢尾花数据集是一个经典数据集，在统计学习和机器学习领域都经常被用作示例。数据集内包含 3 类共 150 条记录，每类各 50 个数据，每条记录都有 4 项特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度，可以通过这4个特征预测鸢尾花卉属于（iris-setosa, iris-versicolour, iris-virginica）中的哪一品种。 三、实验步骤 打开anconda命令行进入合适位置 输入jupyter notebook进入编辑环境，新建python3文件 读入数据 123456# 导入工具库import pandas as pd#读入数据集df = pd.read_csv("iris.data", sep=",", header=None)df.columns = ['萼片长度', '萼片宽度', '花瓣长度', '花瓣宽度', '实际类别']df 萼片长度 萼片宽度 花瓣长度 花瓣宽度 实际类别 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa … … … … … … 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica 150 rows × 5 columns 也可以用sklearn自带的iris数据集,可以看到更多相关信息 12345# 导入工具库import pandas as pdfrom sklearn.datasets import load_iris#读入数据集iris = load_iris() # 加载sklearn自带的数据集 预览数据集 iris是一个字典，包含了数据、标签、标签名、数据描述等信息。可以通过键来索引对应的值, 1iris.data.shape (150, 4) 1iris.feature_names [‘sepal length (cm)’, ‘sepal width (cm)’, ‘petal length (cm)’, ‘petal width (cm)’] 1iris.target array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])1iris.target_names array([‘setosa’, ‘versicolor’, ‘virginica’], dtype=’&lt;U10’) 1iris.filename ‘C:\ProgramData\Anaconda3\lib\site-packages\sklearn\datasets\data\iris.csv’ 1234567891011import matplotlib.pyplot as plt# 前50个样本的散点图plt.scatter(X[:50, 0], X[:50, 1],color='red', marker='o', label='setosa')# 中间50个样本的散点图plt.scatter(X[50:100, 0], X[50:100, 1],color='blue', marker='x', label='versicolor')# 后50个样本的散点图plt.scatter(X[100:, 0], X[100:, 1],color='green', marker='+', label='Virginica')plt.xlabel('petal length')plt.ylabel('sepal length')plt.legend(loc=2) # 说明放在左上角plt.show() 数据预处理 12345678# 数据采用iris.data# 打乱样本顺序from sklearn.utils import shuffledf = shuffle(df)# 划分特征列和类别列X = df[['萼片长度','萼片宽度','花瓣长度','花瓣宽度']]y = df['实际类别'] 划分数据集 按照训练集 : 测试集 = 2:1的比例划分数据集 123456# 按照训练集 : 测试集 = 2:1的比例划分数据集X_test = X[:50]X_train = X[50:]y_test = y[:50]y_train = y[50:]df_test = df[:50] 12# 查看划分的大小是否正确X_train.shape, y_train.shape ((100, 4), (100,)) 1X_test.shape, y_test.shape ((50, 4), (50,)) 训练模型 12345678# 训练模型 clf: classifier分类器from sklearn import treeclf = tree.DecisionTreeClassifier(max_depth = 4)# 查看模型信息，包含各种剪枝参数，其中criterion='gini'属性表示默认使用CART算法，# splitter特征划分标准，默认值为‘best’。在特征的所有划分点中找出最优的划分点，# 详细参数参考https://blog.csdn.net/y0929/article/details/82686177clf = clf.fit(X_train, y_train)clf DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=’gini’, max_depth=4, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=&apos;deprecated&apos;, random_state=None, **splitter=&apos;best&apos;**) 画出决策树 1234567891011121314# 画出决策树feature_names = ['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']target_names = ['setosa', 'versicolor', 'virginica']import pydotplusfrom IPython.display import Image, displaydot_data = tree.export_graphviz(clf, out_file = None, feature_names = feature_names, class_names = target_names, filled = True, rounded = True )graph = pydotplus.graph_from_dot_data(dot_data)display(Image(graph.create_png())) 在测试集检验决策树的分类效果(准确率) 1clf.score(X_test, y_test) 0.96 交叉验证 123456789101112131415# 导入数据采用iris.data文件，预处理、划分数据集同上# 通过循环不断的改变参数，再利用交叉验证来评估不同参数模型的能力from sklearn.model_selection import cross_val_scoreimport matplotlib.pyplot as pltfrom sklearn import treek_range = range(1,7)cv_scores = []for n in k_range: clf = tree.DecisionTreeClassifier(max_depth = n) scores = cross_val_score(clf, X_train, y_train, cv=10) cv_scores.append(scores.mean())plt.plot(k_range,cv_scores)plt.xlabel('n')plt.ylabel('Accuracy') #通过图像选择最好的参数plt.show() 由图像可以看出n=3时性能最好，确定max_depth = 3 *注: *这里由于分组的随机性并不是确定值，要根据每次的图像决定参数 重新训练模型&amp;画图 12345678910111213# 重新训练模型&amp;画图clf = tree.DecisionTreeClassifier(max_depth = 3)clf = clf.fit(X_train, y_train) # 训练模型dot_data = tree.export_graphviz(clf, out_file = None, feature_names = feature_names, class_names = target_names, filled = True, rounded = True )graph = pydotplus.graph_from_dot_data(dot_data)display(Image(graph.create_png())) ![最终决策树](https://s1.ax1x.com/2020/05/09/YMZ9r8.png) **在测试集检验决策树的分类效果** 12# 在测试集检验决策树的分类效果(准确率)clf.score(X_test,y_test) 0.98 结果要好一些 保存测试集结果 1234result = clf.predict(X_test)# 将测试结果转成dataframeresult = pd.DataFrame(&#123;'分类结果':result&#125;)result 分类结果 0 Iris-virginica 1 Iris-versicolor 2 Iris-setosa 3 Iris-virginica 4 Iris-setosa 5 Iris-virginica 6 Iris-versicolor 7 Iris-setosa 8 Iris-setosa 9 Iris-setosa 10 Iris-virginica 11 Iris-virginica 12 Iris-virginica 13 Iris-virginica 14 Iris-virginica 15 Iris-versicolor 16 Iris-virginica 17 Iris-versicolor 18 Iris-virginica 19 Iris-versicolor 20 Iris-versicolor 21 Iris-setosa 22 Iris-virginica 23 Iris-virginica 24 Iris-versicolor 25 Iris-setosa 26 Iris-virginica 27 Iris-setosa 28 Iris-virginica 29 Iris-versicolor 30 Iris-versicolor 31 Iris-virginica 32 Iris-versicolor 33 Iris-virginica 34 Iris-virginica 35 Iris-setosa 36 Iris-setosa 37 Iris-setosa 38 Iris-virginica 39 Iris-versicolor 40 Iris-virginica 41 Iris-virginica 42 Iris-virginica 43 Iris-setosa 44 Iris-setosa 45 Iris-setosa 46 Iris-versicolor 47 Iris-virginica 48 Iris-versicolor 49 Iris-setosa 12345# 将测试结果与原始数据合并df_test.reset_index(drop=True, inplace=True)df_testdf_test['分类结果'] = result['分类结果']df_test | 萼片长度 | 萼片宽度 | 花瓣长度 | 花瓣宽度 | 实际类别 | 分类结果 | | | -------: | -------: | -------: | -------: | -------: | --------------: | --------------- | | 0 | 7.9 | 3.8 | 6.4 | 2.0 | Iris-virginica | Iris-virginica | | 1 | 5.5 | 2.4 | 3.8 | 1.1 | Iris-versicolor | Iris-versicolor | | 2 | 4.9 | 3.1 | 1.5 | 0.1 | Iris-setosa | Iris-setosa | | 3 | 5.9 | 3.2 | 4.8 | 1.8 | Iris-versicolor | Iris-virginica | | 4 | 5.0 | 3.5 | 1.6 | 0.6 | Iris-setosa | Iris-setosa | | 5 | 6.5 | 3.0 | 5.5 | 1.8 | Iris-virginica | Iris-virginica | | 6 | 5.7 | 2.8 | 4.1 | 1.3 | Iris-versicolor | Iris-versicolor | | 7 | 4.3 | 3.0 | 1.1 | 0.1 | Iris-setosa | Iris-setosa | | 8 | 4.6 | 3.4 | 1.4 | 0.3 | Iris-setosa | Iris-setosa | | 9 | 5.7 | 4.4 | 1.5 | 0.4 | Iris-setosa | Iris-setosa | | 10 | 5.8 | 2.7 | 5.1 | 1.9 | Iris-virginica | Iris-virginica | | 11 | 5.8 | 2.8 | 5.1 | 2.4 | Iris-virginica | Iris-virginica | | 12 | 5.6 | 2.8 | 4.9 | 2.0 | Iris-virginica | Iris-virginica | | 13 | 6.4 | 2.8 | 5.6 | 2.1 | Iris-virginica | Iris-virginica | | 14 | 6.1 | 3.0 | 4.9 | 1.8 | Iris-virginica | Iris-virginica | | 15 | 6.3 | 2.3 | 4.4 | 1.3 | Iris-versicolor | Iris-versicolor | | 16 | 7.3 | 2.9 | 6.3 | 1.8 | Iris-virginica | Iris-virginica | | 17 | 6.2 | 2.2 | 4.5 | 1.5 | Iris-versicolor | Iris-versicolor | | 18 | 5.9 | 3.0 | 5.1 | 1.8 | Iris-virginica | Iris-virginica | | 19 | 5.6 | 3.0 | 4.5 | 1.5 | Iris-versicolor | Iris-versicolor | | 20 | 5.6 | 2.5 | 3.9 | 1.1 | Iris-versicolor | Iris-versicolor | | 21 | 4.9 | 3.1 | 1.5 | 0.1 | Iris-setosa | Iris-setosa | | 22 | 6.3 | 3.4 | 5.6 | 2.4 | Iris-virginica | Iris-virginica | | 23 | 7.2 | 3.0 | 5.8 | 1.6 | Iris-virginica | Iris-virginica | | 24 | 5.0 | 2.3 | 3.3 | 1.0 | Iris-versicolor | Iris-versicolor | | 25 | 5.1 | 3.8 | 1.9 | 0.4 | Iris-setosa | Iris-setosa | | 26 | 7.7 | 3.8 | 6.7 | 2.2 | Iris-virginica | Iris-virginica | | 27 | 5.1 | 3.4 | 1.5 | 0.2 | Iris-setosa | Iris-setosa | | 28 | 6.7 | 3.0 | 5.2 | 2.3 | Iris-virginica | Iris-virginica | | 29 | 6.1 | 2.8 | 4.0 | 1.3 | Iris-versicolor | Iris-versicolor | | 30 | 6.6 | 2.9 | 4.6 | 1.3 | Iris-versicolor | Iris-versicolor | | 31 | 6.7 | 3.3 | 5.7 | 2.1 | Iris-virginica | Iris-virginica | | 32 | 5.6 | 2.9 | 3.6 | 1.3 | Iris-versicolor | Iris-versicolor | | 33 | 6.4 | 2.8 | 5.6 | 2.2 | Iris-virginica | Iris-virginica | | 34 | 6.9 | 3.1 | 5.4 | 2.1 | Iris-virginica | Iris-virginica | | 35 | 5.4 | 3.9 | 1.7 | 0.4 | Iris-setosa | Iris-setosa | | 36 | 5.0 | 3.2 | 1.2 | 0.2 | Iris-setosa | Iris-setosa | | 37 | 5.0 | 3.4 | 1.5 | 0.2 | Iris-setosa | Iris-setosa | | 38 | 6.7 | 3.1 | 5.6 | 2.4 | Iris-virginica | Iris-virginica | | 39 | 6.8 | 2.8 | 4.8 | 1.4 | Iris-versicolor | Iris-versicolor | | 40 | 7.2 | 3.6 | 6.1 | 2.5 | Iris-virginica | Iris-virginica | | 41 | 6.4 | 2.7 | 5.3 | 1.9 | Iris-virginica | Iris-virginica | | 42 | 6.2 | 3.4 | 5.4 | 2.3 | Iris-virginica | Iris-virginica | | 43 | 5.1 | 3.3 | 1.7 | 0.5 | Iris-setosa | Iris-setosa | | 44 | 5.2 | 3.5 | 1.5 | 0.2 | Iris-setosa | Iris-setosa | | 45 | 4.8 | 3.4 | 1.6 | 0.2 | Iris-setosa | Iris-setosa | | 46 | 7.0 | 3.2 | 4.7 | 1.4 | Iris-versicolor | Iris-versicolor | | 47 | 5.8 | 2.7 | 5.1 | 1.9 | Iris-virginica | Iris-virginica | | 48 | 6.0 | 2.9 | 4.5 | 1.5 | Iris-versicolor | Iris-versicolor | | 49 | 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | Iris-setosa | 12# 保存测试集结果df_test.to_csv('df_test_result.csv', index=False)四、实验总结​ 任务一学习并使用了决策树分类，通过实际操作，我知道了利用sklearn包实现决策树的CART算法，还有交叉验证可以用来选择最好的剪枝参数，也可以对模型的结果进行验证评估。 任务二### 一、简介 k-means（k-均值）算法 参考： https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html https://blog.csdn.net/ustbbsy/article/details/80960652 https://blog.csdn.net/github_39261590/article/details/76910689 算法描述 K-means算法是很典型的基于距离的聚类算法，采用距离 作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。 k-means算法特点在于：同一聚类的簇内的对象相似度较高；而不同聚类的簇内的对象相似度较小。 算法优缺点 优点： 1.算法快速、简单; 2.对大数据集有较高的效率并且是可伸缩性的; 3.时间复杂度近于线性，而且适合挖掘大规模数据集。K-Means聚类算法的时间复杂度是O(n×k×t) ,其中n代表数据集中对象的数量，t代表着算法迭代的次数，k代表着簇的数目 缺点： 1．在 K-means 算法中 K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。 2．在 K-means 算法中，首先需要根据初始聚类中心来确定一个初始划分，然后对初始划分进行优化。这个初始聚类中心的选择对聚类结果有较大的影响，一旦初始值选择的不好，可能无法得到有效的聚类结果，这也成为 K-means算法的一个主要问题。 3．从 K-means 算法框架可以看出，该算法需要不断地进行样本分类调整，不断地计算调整后的新的聚类中心，因此当数据量非常大时，算法的时间开销是非常大的。所以需要对算法的时间复杂度进行分析、改进，提高算法应用范围。 k-means++算法（k-means算法初值选取优化方法） k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。 算法步骤： （1）从输入的数据点集合中随机选择一个点作为第一个聚类中心 （2）对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) （3）选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 （4）重复2和3直到k个聚类中心被选出来 （5）利用这k个初始的聚类中心来运行标准的k-means算法 二、软件环境准备Anaconda3 sklearn包 云图数据集cloud.data 数据集介绍 数据量 1024 属性数量 10 属性特征 实数（连续属性） 谷歌机翻如下 数据集信息： 我们建议分析的数据集由1024个向量组成，每个向量包含10个参数。您可以将其视为1024 * 10矩阵。要产生这些矢量，请按以下步骤操作： 1.我们从两张512 * 512的AVHRR图像开始（可见图像为1，红外图像为1） 2.每个图像都被划分为超像素16 * 16，每个超像素像素，我们计算出一组参数： （a）可见：均值，最大值，最小值，均值分布，对比度，熵，第二角动量 （b）IR：均值，最大值，最小值 我们选择用来形成向量的10个参数集是各种约束之间的折衷。实际上，我们仍在为数据向量选择参数。我发送给您的数据集尚未规范化。我们的分类方案要求对数据集进行规范化，但对于您的分类可能并非如此。为了标准化数据，我们计算整个数据集上每个参数的均值和标准差，然后计算每个向量的每个参数的均值和标准差： 范数值=（非标准值-平均值）/ SD，其中 平均值=数据集中该特定参数的平均值 SD =标准偏差….. 三、实验步骤具体运行结果见附件Cloud.ipynb 1. 作业只需关注“CLOUD COVER DB #1”，所以对cloud.data进行处理删掉前面的说明和第二部分数据。 2. 数据以空格为分隔，要先处理成csv格式 12345678# 逐行读入数据，将空格转成逗号，保存为csv格式ls = open("cloud.data").readlines()newTxt = ""for line in ls: newTxt = newTxt + ",".join(line.split()) + "\n"fo = open("cloud.csv", "x")fo.write(newTxt)fo.close() 3. 读入数据 123456# 导入工具库import pandas as pd#读入数据集df = pd.read_csv("cloud.csv", sep=",", header=None)df.columns = ['mean','max','min','mean distibution’,’contrast','entropy','second angular momentum','mean’','max’','min1’]df.head() mean max min mean distibution contrast entropy second angular momentum mean max min 0 3.0 140.0 43.5000 0.0833 862.8417 0.0254 3.8890 163.0 240.0 213.3555 1 3.0 135.0 41.9063 0.0790 690.3291 0.0259 3.8340 167.0 239.0 213.7188 2 2.0 126.0 21.0586 0.0406 308.3583 0.0684 3.1702 174.0 240.0 227.5859 3 4.0 197.0 77.4805 0.0890 874.4709 0.0243 3.9442 155.0 239.0 197.2773 4 7.0 193.0 88.8398 0.0884 810.1126 0.0223 3.9318 150.0 236.0 186.0195 4. k-means聚类 这里我们以contrast和mean.1两个维度进行聚类： 12345678from sklearn.cluster import KMeansimport numpy as np# 设置要聚类的字段X = np.array(df[['contrast','mean1']])# 设置簇数为3,默认k-means++算法clf = KMeans(n_clusters=3)# 将数据代入到聚类模型中clf = clf.fit(X) 5. 查看聚类之后的结果： 1clf.cluster_centers_ array([[ 791.98735711, 166.04901961], ​ [ 178.66887036, 204.85741088], ​ [1607.05208313, 163.96385542]]) 6. 根据肘方法确定簇数k。 肘部法求最佳K值 12345678910111213141516# 肘部法求最佳K值K = range(1, 10)mean_distortions = []for k in K:kmeans = KMeans(n_clusters=k)kmeans.fit(X)mean_distortions.append(sum(np.min( cdist(X, kmeans.cluster_centers_, metric='euclidean'), axis=1)) / X.shape[0])plt.plot(K, mean_distortions, 'bx-')plt.xlabel('k')font = FontProperties(fname=r'c:\windows\fonts\msyh.ttc', size=20)plt.ylabel(u'平均畸变程度', fontproperties=font)plt.title(u'用肘部法确定最佳的K值', fontproperties=font) Text(0.5, 1.0, ‘用肘部法确定最佳的K值’) 7. 比较k-means算法和k-means++算法的误差平方和E 123456789101112131415# 计算两个向量的欧式距离的平方，并返回def distEclud(vecA, vecB): return np.sum(np.power(vecA - vecB, 2))# K-means算sse = 0kmodel = KMeans(n_clusters=2, init='random')kmodel.fit(X)# 簇中心cluster_ceter_list = kmodel.cluster_centers_# 个样本属于的簇序号列表cluster_list = kmodel.labels_.tolist()for index in range(len(X)):cluster_num = cluster_list[index]sse += distEclud(X[index, :], cluster_ceter_list[cluster_num])sse 结果: 81199692.74246562 把init=’random’去掉使用默认的K-means++算法，重新训练，结果一致，不知原因。 也可以用自带的kmodel.inertia_属性，结果一致。 ### 四、实验总结 任务二学习并使用了K-means算法，通过实际操作，我知道了利用sklearn包实现K-means算法进行聚类，以及它与K-means++算法的区别。]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
        <tag>决策树</tag>
        <tag>K-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信小程序开发之表单验证（WxValidate使用）]]></title>
    <url>%2F2020%2F04%2F30%2F%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E4%B9%8B%E8%A1%A8%E5%8D%95%E9%AA%8C%E8%AF%81%EF%BC%88WxValidate%E4%BD%BF%E7%94%A8%EF%BC%89%2F</url>
    <content type="text"><![CDATA[小程序表单验证—WxValidate​ 这学期学习小程序有一段时间了，对于表单的应用之前是手写验证规则，对正则表达式的使用要求很高，这次想做的表单项目不少，找到了一个插件专门验证表单，GitHub地址: https://github.com/skyvow/wx-extend 很好用，但是没深入看代码，暂时只写出用法 ​ 首先引入WxValidate.js文件，在 wx-extend/src/assets/plugins/wx-validate/WxValidate.js 路径下，放到utils文件夹下，在index.js中引入 import WxValidate from &#39;../../utils/WxValidate.js&#39; 在wxml中表单的每一项name要对应，且要绑定数据，在js 文件中加入form表单的绑定。比如， &lt;input name=&#39;name&#39; value=&#39;&#39; class=&#39;input&#39;&gt;&lt;/input&gt; 然后就是最重要的验证规则的书写了 首先要在onLoad函数中加入验证规则函数 1234567891011// onLoad中有多个函数的写法，onLoad函数内写函数名，函数在onLoad外定义onLoad() &#123; this.getuser() this.initValidate()//验证规则函数&#125; //onLoad中只有一个函数的写法onLoad:function()&#123; rules:&#123;&#125; messages:&#123;&#125;&#125; 此处需要注意的是一定要在js文件中onLoad验证规则，否则编译会报checkform is not a function 然后是验证规则和报错规则的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445//报错 showModal(error) &#123; wx.showModal(&#123; content: error.msg, showCancel: false, &#125;) &#125;,//验证函数initValidate() &#123; const rules = &#123; name: &#123; required: true, minlength:2 &#125;, phone:&#123; required:true, tel:true &#125; &#125; const messages = &#123; name: &#123; required: '请填写姓名', minlength:'请输入正确的名称' &#125;, phone:&#123; required:'请填写手机号', tel:'请填写正确的手机号' &#125; &#125; this.WxValidate = new WxValidate(rules, messages) &#125;,//调用验证函数 formSubmit: function(e) &#123; console.log('form发生了submit事件，携带的数据为：', e.detail.value) const params = e.detail.value //校验表单 if (!this.WxValidate.checkForm(params)) &#123; const error = this.WxValidate.errorList[0] this.showModal(error) return false &#125; this.showModal(&#123; msg: '提交成功' &#125;)&#125; 参考文献 https://blog.csdn.net/weixin_41041379/article/details/82017301]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>小程序</tag>
        <tag>表单</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序getLocation定位不准]]></title>
    <url>%2F2020%2F04%2F26%2F%E5%B0%8F%E7%A8%8B%E5%BA%8FgetLocation%E5%AE%9A%E4%BD%8D%E4%B8%8D%E5%87%86%2F</url>
    <content type="text"><![CDATA[小程序getLocation定位不准来自官方文档 注意 工具中定位模拟使用IP定位，可能会有一定误差。且工具目前仅支持 gcj02 坐标。 使用第三方服务进行逆地址解析时，请确认第三方服务默认的坐标系，正确进行坐标转换。 ​ 在开发工具里调试会发现地图里显示的是当地政府的地址，就是因为使用IP定位，而在手机端查看会有很大误差，暂时把type改成gcj02并用“真机调试”可以精确定位。]]></content>
      <categories>
        <category>踩坑</category>
      </categories>
      <tags>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu安装Docker下载太慢]]></title>
    <url>%2F2020%2F04%2F17%2Fubuntu%E5%AE%89%E8%A3%85Docker%E4%B8%8B%E8%BD%BD%E5%A4%AA%E6%85%A2%2F</url>
    <content type="text"><![CDATA[Ubuntu安装Docker下载太慢跟着菜鸟教程和docker官网的步骤，到 sudo apt-get install docker-ce docker-ce-cli containerd.io 这一步时要访问美国官方的库apt很慢 首先尝试了虚拟机共享主机的SSR，虚拟机设为NAT连接，在物理机上查询虚拟机ip， 1234567以太网适配器 VMware Network Adapter VMnet8: 连接特定的 DNS 后缀 . . . . . . . : 本地链接 IPv6 地址. . . . . . . . : fe80::9c74:ea9b:8f49:1e96%22 IPv4 地址 . . . . . . . . . . . . : 192.168.199.1 子网掩码 . . . . . . . . . . . . : 255.255.255.0 默认网关. . . . . . . . . . . . . : ssr–&gt;选项设置–&gt;本地代理–&gt;勾选允许来自局域网的连接 Ubuntu下设置–&gt;网络–&gt;网络代理 手动，IP填写上面查到的地址，端口1080 实现了ubuntu下的ssr，但apt还是很慢，看到apt要访问的地址如下，在浏览器中访问 获取:1 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:19.03.8~3-0~ubuntu-bionic [22.9 MB] 有意思的是在浏览器中下载deb文件速度很快，不清楚原因，地址如下 https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/ 下载三个软件最新的deb，拖到ubuntu下安装，再apt或者docker -v检查没问题。 第一个命令sudo docker run hello-world 还是访问国外官方源，发现阿里云有“容器镜像服务”，创建一个容器仓库后会给一个镜像加速器地址，按说明配置即可。 运行容器：docker run -it 镜像名 /bin/bash 退出容器：exit 或者 Ctrl+P+Q暂时退出 查看所有容器：docker ps -a 查看运行的容器：docker ps 重启容器：docker restart 容器ID 重启容器后进入交互式：docker start -i 5c6ce895b979 进入容器：docker attach 容器ID ​ docker exec -it 容器ID /bin/bash docker run -p 80 --name web -i -t ubuntu /bin/bash 启动一个docker容器后发现没有vim等编辑器，apt update还是国外源，和平时换源过程一样，不过修改sources.list文件要先删除原来的行，可以删掉重建sources.list文件，再用echo和&gt;&gt;重定向写到文件中，再update就好了。 12345678910deb http://mirrors.aliyun.com/ubuntu/ trusty main multiverse restricted universedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main multiverse restricted universedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main multiverse restricted universedeb http://mirrors.aliyun.com/ubuntu/ trusty-security main multiverse restricted universedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main multiverse restricted universedeb-src http://mirrors.aliyun.com/ubuntu/ trusty main multiverse restricted universedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main multiverse restricted universedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main multiverse restricted universedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main multiverse restricted universedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main multiverse restricted universe 的击杀敌方拉到回复]]></content>
      <categories>
        <category>踩坑</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu18.04更换国内源]]></title>
    <url>%2F2020%2F04%2F16%2FUbuntu18.04%E6%9B%B4%E6%8D%A2%E5%9B%BD%E5%86%85%E6%BA%90%2F</url>
    <content type="text"><![CDATA[Ubuntu本身的源使用的是国内的源，下载速度比较慢，不像CentOS一样yum安装的时候对镜像站点进项选择，所以选择了更换成国内的源。以下内容整合自网络 备份/etc/apt/sources.list文件1mv /etc/apt/sources.list /etc/apt/sourses.list.backup1 在root管理员下新建/etc/apt/sources.list文件并添加以下内容apt比apt-get多了一些命令，比如sudo apt edit-sources可以直接打开上述文件 1234567891011#阿里云源deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 更改完成之后执行以下命令12# apt update# apt upgrade12 其他的一些apt命令12345678910111213141516sudo apt-get update 更新源sudo apt-get install package 安装包sudo apt-get remove package 删除包sudo apt-cache search package 搜索软件包sudo apt-cache show package 获取包的相关信息，如说明、大小、版本等sudo apt-get install package --reinstall 重新安装包sudo apt-get -f install 修复安装sudo apt-get remove package --purge 删除包，包括配置文件等sudo apt-get build-dep package 安装相关的编译环境sudo apt-get upgrade 更新已安装的包sudo apt-get dist-upgrade 升级系统sudo apt-cache depends package 了解使用该包依赖那些包sudo apt-cache rdepends package 查看该包被哪些包依赖sudo apt-get source package 下载该包的源代码sudo apt-get clean &amp;&amp; sudo apt-get autoclean 清理无用的包sudo apt-get check 检查是否有损坏的依赖12345678910111213141516 其他几个国内的源：1234567891011121314151617181920212223242526272829303132333435#中科大源deb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse#163源deb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse1234567891011#清华源deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于BOA搭建及移植web服务器]]></title>
    <url>%2F2020%2F04%2F16%2F%E5%9F%BA%E4%BA%8EBOA%E6%90%AD%E5%BB%BA%E5%92%8C%E7%A7%BB%E6%A4%8D%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[基于BOA搭建及移植web服务器​ 这学期的硬件课设选择了web服务器。在web服务器中，较为常用的是tomcat，nigix。但是这种服务器比较大，占用资源比较多，并不适合于嵌入式设备中。而boa是一个很轻便的web服务器，部署简单，占用资源少，支持多种语言。 一、下载及配置BOA服务器1. 下载安装boa服务器 ubuntu下操作：首先在http://www.boa.org下载BOA服务器的源码：boa-0.94.13.tar.gz版本为0.94.13。 对其进行解压： $ tar xzf boa-0.94.13.tar.gz 还要安装必要的工具bison，flex。否则会出现： 12make: yacc：命令未找到make: *** [y.tab.c] 错误 127。 $ sudo apt-get install bison flex 打开解压出的文件夹 2. 修改相关配置文件修改src/compat.h文件: 打开compat.h找到这一条语句： #define TIMEZONE_OFFSET(foo) foo##-&gt;tm_gmtoff 将其修改为： #define TIMEZONE_OFFSET(foo) (foo)-&gt;tm_gmtoff 这是由于本机所使用的交叉编译版本对语句用法的不同。 修改 src/log.c文件。 打开log.c注释掉下列语句： 123/*if (dup2(error_log, STDERR_FILENO) == -1) &#123; DIE("unable to dup2 the error log"); &#125;*/ 否则会出现错误： log.c:73 unable to dup2 the error log:bad file descriptor。 修改src/boa.c文件。 打开src/boa.c注释掉下面两句话： 12345678#if 0 if (passwdbuf == NULL) &#123; DIE(”getpwuid”); &#125; if (initgroups(passwdbuf-&gt;pw_name, passwdbuf-&gt;pw_gid) == -1) &#123; DIE(”initgroups”); &#125;#endif 否则会出现错误： boa.c:211 - getpwuid: No such file or directory。 123456#if 0 if (setuid(0) != -1) &#123; DIE(”icky Linux kernel bug!”); &#125;#endif 否则会出现问题： boa.c:228 - icky Linux kernel bug!: No such file or directory。 3. 生成boa可执行文件进入相关目录生成makefile文件： $ cd boa-0.94.13/src $ ./configure 修改makefile文件。 $vim Makefile 修改CC ＝ gcc 为 CC ＝ arm-linux-gnueabihf-gcc 修改CPP ＝ gcc -E 为 CPP ＝ arm-linux-gnueabihf-gcc -E 接下来进行编译。 $ make 然后为刚刚生成的二进制文件boa瘦身删除其调试信息。 $ arm-linux-gnueabihf-strip boa 4. 修改配置找到配置文件boa.conf并进行如下修改： $ sudo vi boa.conf (1)对Group的修改 将Group nogroup（这是修改程序所属的组）。 修改为 Group 0 (2)对user的修改 将User nobody（这里是修改程序所属的用户）。 修改为 User 0 (3)对ScriptAlias的修改 将ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ 修改为 ScriptAlias /cgi-bin/ /www/cgi-bin/（这里是配置服务器读取cgi程序的目录，需要在SD卡中同样的位置建立同样的目录）。 (4)对DoucmentRoot的修改 将DoucmentRoot /var/www 修改为DoucmentRoot /www（这里是服务器初始网页放置的位置同样需要在SD卡同样的位置建立同样的文件夹并将名为index.html的网页放置在其中）。 (5)对ServerName的修改 将#ServerName [www.your.org.here](http://www.your.org.here) 修改为 ServerName www.your.org.here 否则会出现错误“gethostbyname::No such file or directory” (6)对AccessLog修改 将AccessLog /var/log/boa/access_log（在SD卡相应位置建立同名文件夹以存放日志文件，否则提示找不到文件夹，这里不用log功能就注释掉）。 修改为#AccessLog /var/log/boa/access_log 否则会出现错误提示：“unable to dup2 the error log: Bad file descriptor” 在目标板(这里使用树莓派)上需要做的配置： 创建目录/etc/boa并且把boa 和 boa.conf拷贝到这个目录下： 先把文件从虚拟机中拖拽到物理机上，再通过vnc viewer的文件传输功能下载到板子上。用户权限无法查看根目录的文件，使用root权限进行下面操作。 $ mkdir /etc/boa $ cp boa.conf /rootfs/etc/boa 创建HTML文档的主目录/www： $ mkdir /www 创建CGI脚本所在的目录 /www/cgi-bin： $ mkdir /www/cgi-bin 这样boa的服务器基础就搭建好了。 二、 测试服务器动态网页功能​ 到此为止服务器已经搭建完成了，接下来就要测试服务器是否能够成功运行动态网页了，为了顺利完成测试，首先要做的是让目标板和主机能够通信。 ​ 用交叉网线连接主机和开发板的配置较复杂，本地连接要使主机和开发板在同一网段下。折腾失败过一次没再研究。这里使用笔记本和树莓派wifi连在同一路由器下可直接连通，用手机热点效果一样，ping路由器后台查到的ip即可。ping通就可以通信了 接下来要做的就是编译一个cgi程序，因为只是测试用所以就用最简单的hello world来测试： 首先编写hello world代码： 123456789101112#include&lt;stdio.h&gt; int main(int argc, char** argv) &#123; printf("Content-type:text/html\n\n"); printf("&lt;html&gt;\n"); printf("&lt;head&gt;&lt;title&gt;cgiCHello.c&lt;/title&gt;&lt;/head&gt;\n"); printf("&lt;body&gt;\n"); printf("&lt;h1&gt;Hello World! &lt;font color=\"blue\"&gt;\"CGI C\"&lt;/font&gt; &lt;/h1&gt;\n"); printf("&lt;/body&gt;\n"); printf("&lt;/html&gt;\n"); return 0; &#125; 编写完成后将文本文档命名为：hello.c。然后再对其进行编译，使之成为一个cgi文件： arm-linux-gnueabihf-gcc -o hello.cgi hello.c 安装交叉编译器 这里才发现自己的ubuntu没有安装交叉编译器，先在arm-linux-gnueabihf官网 下载压缩包 (1) 拷贝gcc-linaro-6.2.1-2016.11-x86_64_arm-linux-gnueabihf.tar到ubuntu目录下 (2) 在/usr/local/下建立arm文件夹 mkdir /usr/local/arm (3) 进入gcc-linaro-6.2.1-2016.11-x86_64_arm-linux-gnueabihf.tar所在的目录，将gcc-linaro-6.2.1-2016.11-x86_64_arm-linux-gnueabihf.tar解压到/usr/local/arm/下 tar xvf -C/usr/local/arm/ (4) 添加环境变量 vim /etc/profile 在末尾添加 export PATH=$PATH:/usr/local/arm/gcc-linaro-6.2.1-2016.11-x86_64_arm-linux-gnueabihf/bin (5) source /etc/profile 使新添加的环境变量生效 (6) 查看是否安装成功 终端输入arm-linux-gnueabihf-gcc -v出现一系列说明，末尾的版本号与安装版本相同则安装成功 代码的编辑编译可以在树莓派上进行，完全版系统镜像自带gcc gcc -o hello.cgi hello.c 在当前文件夹下便会生成hello.cgi，为这个文件添加权限，使其可执行： chmod +X hello.cgi 将这个文件移动到服务器中的文件夹： cp hello.cgi /www/cgi-bin/ 现在可以打开浏览器在地址栏中输入 板子ip/cgi-bin/hello.cgi 出现下图所示页面说明cgi功能可以使用，可以进行下一步工作了： 动态网页测试 # 实现嵌入式web远程控制功能 ​ 目前为止，服务器的搭建以及数据的传输等基本功能已经实现了，为了能够实现嵌入式web远程控制的功能，就需要让控制者进入到其主页来进行编辑提交，所以在这个页面中就要让控制者能够对目标板进行各种控制和修改，而html只是单方向的，不能进行信息的交互，这时候就需要cgi程序来完成交互的任务。 一、静态网页的配置服务器的文件结构：/www为服务器根目录，存放index.html是服务器的首页，cgi-bin文件夹存放各种cgi程序和sh脚本，下面每个编译好的.cgi和.sh文件都cp到这个目录下。 html文件的格式：编辑index.html，文件的内容如下： 12345678910111213141516171819202122232425262728&lt;!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt; &lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8"&gt; &lt;title&gt;Hello&lt;/title&gt; &lt;script type="text/javascript"&gt; function MM_jumpMenu(targ,selObj,restore)&#123; //v3.0 eval(targ+".location='"+selObj.options[selObj.selectedIndex].value+"'"); if (restore) selObj.selectedIndex=0; &#125; &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello&lt;/h1&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;form id="form1" name="form1" method="get" action="/cgi-bin/reboot.cgi"&gt; &lt;input type="submit" value="重启"&gt; &lt;/form&gt; &lt;a href="http://192.168.43.133/cgi-bin/hello.cgi#tips" target="_blank"&gt;hello~&lt;/a&gt;&lt;pre&gt;&lt;form action="/cgi-bin/Changeip.cgi" method="post"&gt;&lt;input type="text" name="var_ip"&gt;&lt;input type="submit" &gt;&lt;/form&gt;&lt;/p&gt; &lt;/html&gt; 每个标记都是一一对应的。每个网页文件都是以开始和以结束。与之间的内容是网页的标题。会显示在浏览器的标题栏上。与之间放的是网页内容。 显示出的效果如下： index.html `&nbsp;` 语句能够实现空行功能，以便使网页整体结构不会过于紧凑， `hello~` 该语句表示了在网页中加入一个超链接，该链接指向http://192.168.43.133/cgi-bin/hello.cgi 显示为hello用于测试服务器的cgi功能。 点击hello~之后服务器跳转，页面显示的内容： 超链接hello.cgi ## 二、动态网页的配置 1 HTML表单1234&lt;form action="/cgi-bin/Changeip.cgi" method="post"&gt; /表单动作指向Changeip.cgi 传送方式为post&lt;input type="text" name="var_ip"&gt; /插入文本框，赋值名为var_ip&lt;input type="submit" &gt; /插入提交按钮 &lt;/form&gt; /表单结束 上面的语句表示了以post的方式将表单内容发送给Changeip.cgi，同时定义了一个按键和一个输入文本框，按钮的属性为提交按钮，文本框内的值为var_ip的值。 2 cgi功能的实现​ 为了能够实现更改ip的的功能就需要写一个shell脚本调用配置文件并对其进行修改，然而修改这个文件需要root权限，可是单单给shell脚本加上权限并不能达到目的，这时就需要借用一个c程序来提升其权限： Changeip.c: 12345678910111213141516#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int main()&#123; printf("Content-type: text/html\n\n"); //输出类型 uid_t uid ,euid; uid = getuid() ; euid = geteuid(); if(setreuid(euid, uid)) perror("setreuid"); //交换这两个id system("/www/cgi-bin/Changeip.sh"); //调用程序 return 0;&#125; 这个c程序所做的便是提升changip.sh脚本的权限。当在网页中输入一个ip地址后，进入changip.sh 将通过awk截取变量$f 以-为分隔符的第二个域($2)的值，并赋值给var_ip这个变量： 1var_ip=`echo $QUERY_STRING | awk -F '&amp;' '&#123;print $1&#125;' | awk -F '=' '&#123;print $2&#125;'` 因为采用wlan连接，设置静态ip地址不是在interfaces里，而是在/etc/dhcpcd.conf中更改。 将得到的var_ip这个值写入/etc/dhcpcd.conf： 123eth0_cfg="/etc/dhcpcd.conf"sed -i '/^static ip_address=/d' $eth0_cfg #删除匹配到ip的行echo "static ip_address=$var_ip/24" &gt;&gt; $eth0_cfg 这样就完成了ip的更改 Changip.sh: 12345678910111213#!/bin/bashif [ $REQUEST_METHOD = "POST" ]; then QUERY_STRING=`cat /dev/stdin`fiecho "Content-type:text/html"echo ""eth0_cfg="/etc/dhcpcd.conf"var_ip=`echo $QUERY_STRING | awk -F '&amp;' '&#123;print $1&#125;' | awk -F '=' '&#123;print $2&#125;'`sed -i '/^static ip_address=/d' $eth0_cfgecho "static ip_address=$var_ip/24" &gt;&gt; $eth0_cfg 使开发板重启功能也是通过cgi调用sh脚本实现。 仿照Changeip编写如下代码： reboot.c: 1234567891011121314151617181920212223#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int main()&#123; printf("Content-type: text/html\n\n"); printf("&lt;html&gt;\n"); printf("&lt;head&gt;&lt;title&gt;reboot&lt;/title&gt;&lt;/head&gt;\n"); printf("&lt;body&gt;\n"); printf("&lt;h1&gt;The system is going to reboot .......\n\n &lt;/h1&gt;\n"); printf("&lt;/body&gt;\n"); printf("&lt;/html&gt;\n"); uid_t uid ,euid; uid = getuid() ; euid = geteuid(); if(setreuid(euid, uid)) perror("setreuid"); system("/www/cgi-bin/reboot.sh"); return 0;&#125; reboot.sh: 1234#!/bin/bashecho "Content-type:text/html"echo "系统将要重启！"/sbin/shutdown -r now 经过后面的测试shell脚本可以使用，前提是需要使用命令chmod 777 reboot.sh改变其权限。 (这里修改为u+s无效，待+6解) 编译Change.c和reboot.c，将.cgi和.sh文件都cp到/www/cgi-bin下 三、测试远程控制功能1.查看boa服务boa配置好后一般随开机启动 ps -aux | grep boa可以查看boa的进程号，kill进程号关闭，boa命令重启。 启动boa服务器 在主机浏览器地址栏内输入开发板服务器的ip（192.168.43.133）这样就进入了服务器的主页，主页的内容很直接，两个按钮，一个超链接，一个文本框： boa服务器主页 2.测试修改ip首先设置板子的静态ip vim /etc/dhcpcd.conf 在文件末尾添加静态地址配置 1234interface wlan0static routers=192.168.1.1static domain_name_servers=114.114.114.114 8.8.8.8static ip_address=192.168.43.133/24 重启wifi网卡 wpa_cli -i wlan0 reconfigure 查看开发板当前ip地址，输入ifconfig wlan0或ip addr show wlan0命令后可以看到板子当前的连接，硬件地址，ipv4和ipv6地址，广播地址，以及子网掩码等。 当前ip地址 在网页文本框输入想要修改的ip地址点击提交，正常弹出网页后ifconfig wlan0查看板子ip，这时并没有更改，查看dhcpcd.conf文件，发现已经修改成功，因为更改conf文件后要再次重启wifi网卡，发现远程登陆断开，以新ip登陆，这时查看ip已经改变为刚才输入的地址了。 3.测试重启功能在网页上点击重启按钮，弹出窗口，远程连接断开，板子灯表示正在重启，等待一会恢复连接。 总结​ 通过这次课设第一次搭建服务器，了解服务器文件的结构和运行方式，学到了更多linux下的命令，遇到问题多查官方文档和其他个人博客，踩到的雷前人一般都有解决方法。对于boa和cgi程序有了初步了解，尤其是权限问题，cgi和sh脚本经常提高权限才能正常运行。]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>入门</tag>
        <tag>boa</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识树莓派记录]]></title>
    <url>%2F2020%2F04%2F16%2F%E5%88%9D%E8%AF%86%E6%A0%91%E8%8E%93%E6%B4%BE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[初识树莓派记录​ 这个学期有一个智能硬件系统开发综合实践，从暑假看到树莓派4发布就想搞一个玩，这次正好有机会就买了一个，淘宝购入2g无卡版，包含板子、第三方5V3A电源、外壳和一根micro-HDMI转HDMI线。另配一张64g存储卡。 一、开箱贴散热片，按照卖家给的教程安装外壳，烧系统，添加SSH和WiFi配置文件，putty登录开启VNC，设置分辨率，否则无法连接VNC，通过路由器后台查看板子IP，VNC Viewer登录进图形界面。卖家提供的资料如下 树莓派4B带风扇外壳安装视频链接：https://pan.baidu.com/s/1iowbKNnrdztEK2sj562peA 提取码：pshs 树莓派4系统烧入及WIFI配置方法说明： https://pan.baidu.com/s/1knP0QzTLYMBvj_bLE7M1Eg 提取码：38ta 二、初始配置安装的系统是基于Debian的树莓派官方系统，Linux常用命令通用，注意用户权限。 1. 更换国内镜像默认镜像源在国外，安装软件下载很慢，经常碰到ip超时，一查是伦敦的ip，换到国内（这里采用清华大学的源）就好很多。 1.1. 编辑sources.list 打开终端 输入 sudo nano /etc/apt/sources.list 用#注释或直接删除原有的内容，新增两条： 12deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi#deb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi ctrl+x 保存并退出。 1.2编辑raspi.listsudo nano /etc/apt/sources.list.d/raspi.list 用#注释或直接删除原有的内容，新增两条： 12deb http://mirror.tuna.tsinghua.edu.cn/raspberrypi/ stretch main uideb-src http://mirror.tuna.tsinghua.edu.cn/raspberrypi/ stretch main ui ctrl+x 保存并退出。 更新软件源列表：sudo apt-get update 2. 重新安装vim树莓派自带的vim很不好用，卸掉重装 12sudo apt-get remove vim-commonsudo apt-get install vim 安装成功后可以添加一些功能 sudo vi /etc/vim/vimrc 在文件尾部加入 123set nu #显示行号syntax on #语法高亮set tabstop=4 #tab退四格 这里加注释会使每次打开vim报警告，不用写。vim的复制粘贴为y和p，在命令模式下选中待操作文本按y即复制，p即粘贴。 3.安装中文输入法sudo apt-get install scim-pinyin]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机不能复制粘贴]]></title>
    <url>%2F2020%2F04%2F16%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%8D%E8%83%BD%E5%A4%8D%E5%88%B6%E7%B2%98%E8%B4%B4%2F</url>
    <content type="text"><![CDATA[虚拟机不能复制粘贴经检查是VM tools的问题，右键虚拟机看到安装VM tools 按钮是灰色的，要在虚拟机设置里把CD/DVD CD/DVD(2)和软盘设为自动检测 重启虚拟机 这时可以点安装VM tools，点击后会在桌面产生一个光盘镜像，点击后把安装包复制到home目录下 解压安装文件tar - -zxvf 进入目录cd vmware-tools-distrib 执行安装./vmware-install.pl 重启虚拟机，可以互相复制粘贴]]></content>
      <categories>
        <category>踩坑</category>
      </categories>
      <tags>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘大作业1记录]]></title>
    <url>%2F2020%2F04%2F05%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-FP-growth%2F</url>
    <content type="text"><![CDATA[数据挖掘-FP-growth一、简介​ 本项目是基于FP-growth算法的淘宝用户数据关联分析，使用mlxtend包中的FP-growth算法实现数据的频繁模式挖掘并分析 使用的数据集：阿里天池 User Behavior Data from Taobao for Recommendation https://tianchi.aliyun.com/dataset/dataDetail?dataId=649 频繁模式和关联规则​ 大量数据中的频繁模式、关联和相关关系的发现，在选中市场、决策分析和商务关联方面是有用的。一个流行的应用领域是购物篮分析，通过搜索经常一块（或依次）购买的商品的集合，研究顾客的购买习惯，以发现一些隐藏的、有趣的规则。典型的如顾客购买啤酒的时候很有可能会购买尿布。关联规则挖掘首先找出频繁项集（项的集合，如A和B，满足最小支持度阀值，或任务相关元组的百分比），然后，由它们产生形如A=&gt;B的强关联规则。这些规则也满足最小置信度阀值（预定义的、在满足A的条件下满足B的概率）。进一步分析关联，发现项集A和B之间具有统计相关的相关规则。 ​ 对于频繁模式的挖掘，已有许多有效的、可伸缩的算法，由它们可以导出关联和相关规则，书上介绍了Apriori和FP-growth两种算法，在mlxtend包中都有，我决定使用FP-growth算法 FP-growth算法​ FP-Growth(频繁模式增长算法是韩嘉炜等人在2000年提出的关联分析算法，它采取如下分治策略：将提供频繁项集的数据库压缩到一棵频繁模式树（FP-tree），但仍保留项集关联信息。在算法中使用了一种称为频繁模式树（Frequent Pattern Tree）的数据结构。FP-tree是一种特殊的前缀树，由频繁项头表和项前缀树构成。FP-Growth算法基于以上的结构加快整个挖掘过程。 ​ 相比Apriori算法需要多次扫描数据库，FP-growth只需要对数据库扫描2次。第1次扫描获得当个项目的频率，去掉不满足支持度要求的项，并对剩下的项排序。第2次扫描建立一颗FP-Tree树。 挖掘频繁模式前首先要构造FP-Tree，输入一个交易数据库DB和一个最小支持度threshold，输出:它的FP-tree. 二、软件环境准备Visual Studio Code Anaconda3 在Anaconda中安装pandas和mlxtend conda install -c conda-forge mlxtend conda install -c conda-forge pandas mlxtend-master程序源码（https://github.com/rasbt/mlxtend） 淘宝的用户行为数据集（https://tianchi.aliyun.com/dataset/dataDetail?dataId=649） 使用pip安装包时使用国内源，否则可能超时报错 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple mlxtend 在使用vs2019安装python后要设置环境变量，在系统变量&gt;path中添加C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\Scripts，否则会提示pip等不是命令 三、基本思路 读懂mlxtend中的FP-growth算法部分，了解其原理和大致过程 在jupyter notebook中建立python3文件 通过Pandas读入数据，首先对数据进行观察：有无索引、标号、有没有数据缺失 数据预处理 频繁模式挖掘 结果分析 四、算法分析 在vscode中打开mlxtend-master文件夹 mlxtend-master\docs\sources\user_guide\frequent_patterns路径下可以找到介绍文档fpgrowth.ipynb，最好用jupyter notebook打开。 mlxtend-master\mlxtend\frequent_patterns路径下可以找到FP-growth和挖掘关联规则相关代码， 主要涉及三个文件的代码，fpgrowth.py是最后用到的主函数，其中调用的大多数函数在fpcommon.py中，输入一个数据集、最小支持度、是否使用列名、最大频繁度、是否显示树生成过程，返回支持度和频繁项集的列表。association_rules.py是关联规则挖掘的算法。 实际过程只用两个函数 fpgrowth(df, min_support, use_colnames, max_len, verbose): association_rules(df, metric, min_threshold, support_only): 代码的具体分析见附录，之前没有python基础，根据自己理解进行了注解(谷歌机翻)，可能有误。 FP-growth简要步骤 检查数据，有错就抛出异常 将数据的列名组合为一个索引序列 调用setup_fptree建立FP树， 判断数据是否稀疏，计算每项的支持度 去掉支持度小于最小支持度的项，生成频繁1项的一维列表 定义要插入FP树的项目的排序 构建FP树，包含根节点、节点、子树和排序，每个节点包含内容、出现次数、父节点和子节点 构建FP树过程参考此视频 https://www.bilibili.com/video/BV1LJ411W7rD 计算支持度计数 执行fpgrowth算法的递归步骤。 如果树只有一条路径，可以组合生成所有剩余项集 生成子树以生成更大的频繁项集 生成支持度和项集的列表 挖掘关联规则​ 搬运自教材P164 ​ 一旦由数据库D中的事务找出频繁项集，就可以直接由它们产生强关联规则（强关联规则满足最小支持度和最小置信度）。对于置信度，可以用下式计算。 confidence(A=&gt;B)= P(A|B) = support_count(AUB) / support_count(A) ​ 条件概率用项集的支持度计数表示，其中， support_count（AUB）是包含项集AUB的事务数，而 support_count(A)是包含项集A的事务数。根据该式，关联规则可以产生如下： 对于每个频繁项集l，产生l的所有非空子集。 对于l的每个非空子集s，如果support_count(t) / support_count(s)≥min_conf,则输出规则”s=(l-s)。” 其中，min_conf是最小置信度阈值。 ​ 由于规则由频繁项集产生，因此每个规则都自动地满足最小支持度。频繁项集和它们的支持度可以预先存放在散列表中，使得它们可以被快速访问。 mlxtend示例 http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/ mlxtend使用了 DataFrame 方式来描述关联规则，而不是 —&gt; 符号，其中： antecedents：规则先导项 consequents：规则后继项 antecedent support：规则先导项支持度 consequent support：规则后继项支持度 support：规则支持度 （前项后项并集的支持度） confidence：规则置信度 （规则置信度：规则支持度support / 规则先导项） lift：规则提升度，表示含有先导项条件下同时含有后继项的概率，与后继项总体发生的概率之比。 leverage：规则杠杆率，表示当先导项与后继项独立分布时，先导项与后继项一起出现的次数比预期多多少。 conviction：规则确信度，与提升度类似，但用差值表示。 五、实验步骤 打开anconda命令行进入合适位置 输入jupyter notebook进入编辑环境，新建python3文件 数据预处理 1import pandas as pd # 导入Pandas 12data = pd.read_csv('data_buy.csv', header=None) # 读入解压好的 csv 文件到 data 变量中#这里pd.read_csv是将数据一次性读入内存，可以使用分块读取等方法 1data.info() # 先看一下读入的数据信息 123456789101112&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 100150807 entries, 0 to 100150806Data columns (total 5 columns): # Column Dtype --- ------ ----- 0 0 int64 1 1 int64 2 2 int64 3 3 object 4 4 int64 dtypes: int64(4), object(1)memory usage: 3.7+ GB 1data.head() # 查看数据前5行 0 1 2 3 4 0 1 2268318 2520377 pv 1511544070 1 1 2333346 2520771 pv 1511561733 2 1 2576651 149192 pv 1511572885 3 1 3830808 4181361 pv 1511593493 4 1 4365585 2520377 pv 1511596146 现在表格没有列名称，不方便处理，我们可以手动添加 12# 用户ID，商品ID，商品类目ID，行为类型，时间戳data.columns = ['user', 'item', 'category', 'behavior', 'timestamp'] 1data.head() # 再来看一下数据 | | user | item | category | behavior | timestamp | | ---: | ---: | ------: | -------: | -------: | ---------: | | 0 | 1 | 2268318 | 2520377 | pv | 1511544070 | | 1 | 1 | 2333346 | 2520771 | pv | 1511561733 | | 2 | 1 | 2576651 | 149192 | pv | 1511572885 | | 3 | 1 | 3830808 | 4181361 | pv | 1511593493 | | 4 | 1 | 4365585 | 2520377 | pv | 1511596146 | 123# 将时间戳转换为时间格式,读入数据按秒计算data['time']=pd.to_datetime(data['timestamp'],unit='s')data.head() user item category behavior timestamp time 0 1 2268318 2520377 pv 1511544070 2017-11-24 17:21:10 1 1 2333346 2520771 pv 1511561733 2017-11-24 22:15:33 2 1 2576651 149192 pv 1511572885 2017-11-25 01:21:25 3 1 3830808 4181361 pv 1511593493 2017-11-25 07:04:53 4 1 4365585 2520377 pv 1511596146 2017-11-25 07:49:06 1234# 提取出时间的小时data = data.drop(columns=['timestamp'])data['daily'] = data['time'].dt.hourdata.head() | | user | item | category | behavior | time | daily | | ---: | ---: | ------: | -------: | -------: | ------------------: | ----: | | 0 | 1 | 2268318 | 2520377 | pv | 2017-11-24 17:21:10 | 17 | | 1 | 1 | 2333346 | 2520771 | pv | 2017-11-24 22:15:33 | 22 | | 2 | 1 | 2576651 | 149192 | pv | 2017-11-25 01:21:25 | 1 | | 3 | 1 | 3830808 | 4181361 | pv | 2017-11-25 07:04:53 | 7 | | 4 | 1 | 4365585 | 2520377 | pv | 2017-11-25 07:49:06 | 7 | 123# 分组统计每个时间段发生的各种行为table_time = data.groupby(['daily','behavior']).size().unstack()table_time.head() behavior buy cart fav pv daily 0 64917 192036 103721 3043136 1 96134 229890 127976 3729408 2 127933 266963 147752 4335949 3 122048 260831 145412 4214991 4 118591 255811 140862 4257521 12# 画出柱状图，非堆叠，横轴标注不旋转，规定尺寸table_time.plot.bar(stacked=False,rot=0,figsize=(16,7)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x28e81de33c8&gt; ​ 用户行为有四种：pv（点击）、cart（加购）、fav（收藏）、buy（购买） ​ 从图中可以看到 ，用户行为高峰在中午时间段，上午相对比较平稳，下午两点后开始下降到九点， 数据集包含了2017年11月25日至2017年12月3日之间,有两个周末，可能使用户活跃时间偏早。部分用户在深夜活跃，可能与商家活动有关。但是由于pv量显著高于其他用户行为的发生，因此需要根据不同行为进行研究。 123# 去掉pv量后重新画图table_time1 = table_time.drop(columns=['pv'])table_time1.plot.bar(stacked=False,rot=0,figsize=(16,7)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x28e8215f848&gt; ​ 可以看到用户的其他行为趋势大致相同 ​ 我们的目的是挖掘用户购买商品类别间的频繁模式和关联规则，下面开始正式的数据预处理。 123# 去除具体商品信息，时间信息，只看用户对某类商品的操作数据data=data.drop(columns=['item', 'time', 'daily'])data.head() user category behavior 0 1 2520377 pv 1 1 2520771 pv 2 1 149192 pv 3 1 4181361 pv 4 1 2520377 pv 12345# 若只关注用户购买行为data=data[data['behavior']=='buy']# 按照用户和商品种类去重data=data.drop_duplicates(['user','category']).copy()data.head() user category behavior 71 100 2951233 buy 73 100 4869428 buy 100 100 2429887 buy 119 100 3002561 buy 125 100 4098232 buy 123# 先转换DataFrame数据为包含数据的列表，后续挖掘时再转换回来data_list = data.groupby('user')['category'].apply(list)data_list[:5] # 查看列表前5行 user2 [2865017, 1849958, 2925160, 3439012, 2885642, …4 [2465336, 4145813, 4801426]11 [3102419]16 [3248072, 3898483]17 [3702593]Name: category, dtype: object 12transactions = list(data_list)transactions[:5] [[2865017, 1849958, 2925160, 3439012, 2885642, 4159072], [2465336, 4145813, 4801426], [3102419], [3248072, 3898483], [3702593]] 123# 保存清洗后的数据import jsonjson.dump(transactions, open('transactions.json', 'w')) 频繁模式和关联规则 由于mlxtend的模型只接受特定的数据格式。 TransactionEncoder类似于独热编码，每个值转换为一个唯一的bool值 1234567import pandas as pdimport json# 传入模型的数据需要满足特定的格式，可以用这种方法来转换为bool值，也可以用函数转换为0、1from mlxtend.preprocessing import TransactionEncoder# 导入Apriori算法、fpgrowth算法和导入关联规则from mlxtend.frequent_patterns import apriori, fpgrowthfrom mlxtend.frequent_patterns import association_rules 12# 导入预处理好的数据“transactions.json”dataset = json.load(open('transactions.json')) 123456# 使用FP-Growth算法挖掘频繁项集，最小支持度取值0.001te = TransactionEncoder()te_ary = te.fit(dataset).transform(dataset)df = pd.DataFrame(te_ary, columns=te.columns_)frequent_itemsets = fpgrowth(df, min_support=0.001)frequent_itemsets # 频繁项集 support itemsets 0 0.040330 (3973) 1 0.023215 (5751) 2 0.004299 (4765) 3 0.002152 (4030) 4 0.040941 (5734) … … … 598 0.001825 (3627, 3541) 599 0.001126 (3449, 1790) 600 0.001038 (3449, 3766) 601 0.001208 (3449, 1349) 602 0.001048 (3449, 5734) 603 rows × 2 columns 12# 获取置信度&gt;=0.1的关联规则，并按提升度倒序排列association_rules(frequent_itemsets, metric="confidence", min_threshold=0.1).sort_values('lift', ascending=False) | | antecedents | consequents | antecedent support | consequent support | support | confidence | lift | leverage | conviction | | ---: | ----------: | ----------: | -----------------: | -----------------: | -------: | ---------: | --------: | -------: | ---------: | | 20 | (6020) | (3627) | 0.004004 | 0.021094 | 0.001220 | 0.304606 | 14.440105 | 0.001135 | 1.407700 | | 22 | (3541) | (3627) | 0.007655 | 0.021094 | 0.001825 | 0.238391 | 11.301132 | 0.001663 | 1.285313 | | 14 | (5148) | (3627) | 0.012316 | 0.021094 | 0.002098 | 0.170390 | 8.077478 | 0.001839 | 1.179959 | | 13 | (1421) | (2563) | 0.014085 | 0.015971 | 0.001597 | 0.113399 | 7.100270 | 0.001372 | 1.109889 | | 12 | (2563) | (1421) | 0.015971 | 0.014085 | 0.001597 | 0.100009 | 7.100270 | 0.001372 | 1.095472 | | 11 | (2563) | (1763) | 0.015971 | 0.017494 | 0.001617 | 0.101220 | 5.785993 | 0.001337 | 1.093155 | | 19 | (194) | (1790) | 0.014359 | 0.022164 | 0.001605 | 0.111756 | 5.042266 | 0.001286 | 1.100864 | | 0 | (3973) | (5751) | 0.040330 | 0.023215 | 0.004433 | 0.109927 | 4.735128 | 0.003497 | 1.097421 | | 1 | (5751) | (3973) | 0.023215 | 0.040330 | 0.004433 | 0.190967 | 4.735128 | 0.003497 | 1.186194 | | 15 | (194) | (1349) | 0.014359 | 0.031514 | 0.001979 | 0.137856 | 4.374467 | 0.001527 | 1.123346 | | 8 | (1790) | (1349) | 0.022164 | 0.031514 | 0.002790 | 0.125881 | 3.994464 | 0.002092 | 1.107957 | | 10 | (3235) | (1349) | 0.015650 | 0.031514 | 0.001872 | 0.119643 | 3.796518 | 0.001379 | 1.100106 | | 2 | (3375) | (6609) | 0.016750 | 0.033797 | 0.002137 | 0.127586 | 3.775105 | 0.001571 | 1.107505 | | 9 | (1790) | (6609) | 0.022164 | 0.033797 | 0.002475 | 0.111655 | 3.303741 | 0.001726 | 1.087645 | | 7 | (1349) | (6609) | 0.031514 | 0.033797 | 0.003409 | 0.108164 | 3.200443 | 0.002344 | 1.083387 | | 6 | (6609) | (1349) | 0.033797 | 0.031514 | 0.003409 | 0.100858 | 3.200443 | 0.002344 | 1.077123 | | 4 | (3450) | (6609) | 0.013959 | 0.033797 | 0.001475 | 0.105689 | 3.127213 | 0.001004 | 1.080389 | | 3 | (5831) | (6542) | 0.018534 | 0.035327 | 0.001910 | 0.103033 | 2.916559 | 0.001255 | 1.075484 | | 17 | (194) | (5734) | 0.014359 | 0.040941 | 0.001486 | 0.103470 | 2.527278 | 0.000898 | 1.069745 | | 5 | (1349) | (3766) | 0.031514 | 0.042770 | 0.003267 | 0.103681 | 2.424128 | 0.001920 | 1.067956 | | 16 | (194) | (3766) | 0.014359 | 0.042770 | 0.001452 | 0.101088 | 2.363492 | 0.000837 | 1.064875 | | 21 | (6593) | (2003) | 0.020254 | 0.044546 | 0.002092 | 0.103312 | 2.319203 | 0.001190 | 1.065536 | | 18 | (194) | (2003) | 0.014359 | 0.044546 | 0.001465 | 0.102020 | 2.290203 | 0.000825 | 1.064003 |六、代码分析fpgrowth.py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109import mathimport itertoolsfrom ..frequent_patterns import fpcommon as fpcdef fpgrowth(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0): """从一键式DataFrame获取频繁项集 参数 ----------- df : pandas DataFrame pandas DataFrame的编码格式。 还支持具有稀疏数据的DataFrames 请注意，旧的pandas SparseDataFrame格式在mlxtend&gt; = 0.17.2中不再受支持。 允许的值为0/1或True / False。 举例, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) 0到1之间的浮点数，用于最小程度地支持返回的项目集。 相对支持度=发生项目的交易 / 总交易. use_colnames : bool (default: False) 如果为true，则在返回的DataFrame中使用DataFrame的列名而不是列索引。 max_len : int (default: None) 生成的项目集的最大长度。 如果全部为“无”（默认）,评估可能的项目集长度。 verbose : int (default: 0) 显示条件树生成的阶段。 Returns ----------- pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are &gt;= `min_support` and &lt; than `max_len` (if `max_len` is not None). Each itemset in the 'itemsets' column is of type `frozenset`, 这是Python内置类型，其行为类似于设置，它是不可变的 """ fpc.valid_input_check(df) #检查输入的数据 # 输入的最小支持度&lt;=0，抛出异常 if min_support &lt;= 0.: raise ValueError('`min_support` must be a positive ' 'number within the interval `(0, 1]`. ' 'Got %s.' % min_support) # 将数据的列名组合为一个索引序列 colname_map = None if use_colnames: colname_map = &#123;idx: item for idx, item in enumerate(df.columns)&#125; tree = fpc.setup_fptree(df, min_support) #建立FP树 # 最小支持度计数 = 总数*相对支持度，index返回索引 minsup = math.ceil(min_support * len(df.index)) generator = fpg_step(tree, minsup, colname_map, max_len, verbose) #生成树 return fpc.generate_itemsets(generator, len(df.index), colname_map)def fpg_step(tree, minsup, colnames, max_len, verbose): """ 执行fpgrowth算法的递归步骤。 参数 ---------- tree : FPTree minsup : int 产量 ------ 字符串列表 minsup项目集中已发生的项目集。 """ count = 0 items = tree.nodes.keys() if tree.is_path(): # 如果树只有一条路径，可以组合生成所有剩余项集，而无需生成其他条件树 size_remain = len(items) + 1 if max_len: size_remain = max_len - len(tree.cond_items) + 1 for i in range(1, size_remain): for itemset in itertools.combinations(items, i): count += 1 support = min([tree.nodes[i][0].count for i in itemset]) yield support, tree.cond_items + list(itemset) elif not max_len or max_len &gt; len(tree.cond_items): for item in items: count += 1 support = sum([node.count for node in tree.nodes[item]]) yield support, tree.cond_items + [item] if verbose: tree.print_status(count, colnames) # 生成子树以生成更大的频繁项集 if not tree.is_path() and (not max_len or max_len &gt; len(tree.cond_items)): for item in items: cond_tree = tree.conditional_tree(item, minsup) for sup, iset in fpg_step(cond_tree, minsup, colnames, max_len, verbose): yield sup, iset fpcommon.py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238import numpy as npimport pandas as pdimport collectionsfrom distutils.version import LooseVersion as Versionfrom pandas import __version__ as pandas_versiondef setup_fptree(df, min_support): num_itemsets = len(df.index) #数据库中项目集的数量 # 判断df是否是稀疏的 is_sparse = False if hasattr(df, "sparse"): # 稀疏的DataFrame (pandas &gt;= 0.24) if df.size == 0: itemsets = df.values else: itemsets = df.sparse.to_coo().tocsr() #转换为稀疏的 is_sparse = True else: # 稠密的DataFrame itemsets = df.values # 每个项目的支持度 # 如果项目集稀疏，则np.sum返回形状为（1，N）的np.matrix矩阵 item_support = np.array(np.sum(itemsets, axis=0) / float(num_itemsets)) # 不分行列改成一串 item_support = item_support.reshape(-1) # 去掉小于设定的最小支持度的项，完成频繁一项集 items = np.nonzero(item_support &gt;= min_support)[0] # 定义要插入FPTree的项目的排序 # argsort()返回的是支持度从小到大的索引值。 # enumerate()将数据组合为一个索引序列，完成F-list indices = item_support[items].argsort() rank = &#123;item: i for i, item in enumerate(items[indices])&#125; # 根据顺序插入项目集来构建树 # 为了减少树大小，插入按最频繁到最不频繁的顺序进行 tree = FPTree(rank) for i in range(num_itemsets): if is_sparse: # 项目集已转换为CSR格式，以加快以下行的速度。 它具有3个属性： # - itemsets.data 包含非null值, shape(#nnz,) # - itemsets.indices 包含非空元素的列数, shape(#nnz,) # - itemsets.indptr[i] 包含第i行中第一个非null元素的itemet.indices中的偏移量, shape(1+#nrows,) nonnull = itemsets.indices[itemsets.indptr[i]:itemsets.indptr[i+1]] else: nonnull = np.where(itemsets[i, :])[0] itemset = [item for item in nonnull if item in rank] itemset.sort(key=rank.get, reverse=True) tree.insert_itemset(itemset) return tree, rank# 生成支持度和项集的列表def generate_itemsets(generator, num_itemsets, colname_map): itemsets = [] supports = [] for sup, iset in generator: itemsets.append(frozenset(iset)) supports.append(sup / num_itemsets) res_df = pd.DataFrame(&#123;'support': supports, 'itemsets': itemsets&#125;) if colname_map is not None: res_df['itemsets'] = res_df['itemsets'] \ .apply(lambda x: frozenset([colname_map[i] for i in x])) return res_df# 检查数据，有错抛出异常def valid_input_check(df): if f"&#123;type(df)&#125;" == "&lt;class 'pandas.core.frame.SparseDataFrame'&gt;": msg = ("SparseDataFrame support has been deprecated in pandas 1.0," " and is no longer supported in mlxtend. " " Please" " see the pandas migration guide at" " https://pandas.pydata.org/pandas-docs/" "stable/user_guide/sparse.html#sparse-data-structures" " for supporting sparse data in DataFrames.") raise TypeError(msg) if df.size == 0: return if hasattr(df, "sparse"): if not isinstance(df.columns[0], str) and df.columns[0] != 0: raise ValueError('Due to current limitations in Pandas, ' 'if the sparse format has integer column names,' 'names, please make sure they either start ' 'with `0` or cast them as string column names: ' '`df.columns = [str(i) for i in df.columns`].') # 捷径: 如果所有列均为布尔值，则无需检查 all_bools = df.dtypes.apply(pd.api.types.is_bool_dtype).all() if not all_bools: # Pandas比numpy慢得多，因此在Numpy数组上使用np.where if hasattr(df, "sparse"): if df.size == 0: values = df.values else: values = df.sparse.to_coo().tocoo().data else: values = df.values idxs = np.where((values != 1) &amp; (values != 0)) if len(idxs[0]) &gt; 0: # idxs具有稀疏数据的1维和密集数据的2维 val = values[tuple(loc[0] for loc in idxs)] s = ('The allowed values for a DataFrame' ' are True, False, 0, 1. Found value %s' % (val)) raise ValueError(s)# 构建FP树class FPTree(object): def __init__(self, rank=None): self.root = FPNode(None) self.nodes = collections.defaultdict(list) self.cond_items = [] self.rank = rank def conditional_tree(self, cond_item, minsup): """ 创建并返回以cond_item为条件的self的子树。 参数 ---------- cond_item : int | str 树（自身）将作为条件的项目。 minsup : int 最低支持阈值。 Returns ------- cond_tree : FPtree """ # 查找从根节点到项目节点的所有路径 branches = [] count = collections.defaultdict(int) for node in self.nodes[cond_item]: branch = node.itempath_from_root() branches.append(branch) for item in branch: count[item] += node.count # 定义新的顺序或深层树可能组合爆炸?? items = [item for item in count if count[item] &gt;= minsup] items.sort(key=count.get) rank = &#123;item: i for i, item in enumerate(items)&#125; # 创建条件树 cond_tree = FPTree(rank) for idx, branch in enumerate(branches): branch = sorted([i for i in branch if i in rank], key=rank.get, reverse=True) cond_tree.insert_itemset(branch, self.nodes[cond_item][idx].count) cond_tree.cond_items = self.cond_items + [cond_item] return cond_tree def insert_itemset(self, itemset, count=1): """ 将项目列表插入树中。 参数 ---------- itemset : list 将插入树中的项目。 count : int 项目集的出现次数。 """ self.root.count += count if len(itemset) == 0: return # 尽可能遵循树中的现有路径 index = 0 node = self.root for item in itemset: if item in node.children: child = node.children[item] child.count += count node = child index += 1 else: break # 插入所有剩余的项目 for item in itemset[index:]: child_node = FPNode(item, count, node) self.nodes[item].append(child_node) node = child_node #判断树是否是一条路径 def is_path(self): if len(self.root.children) &gt; 1: return False for i in self.nodes: if len(self.nodes[i]) &gt; 1 or len(self.nodes[i][0].children) &gt; 1: return False return True def print_status(self, count, colnames): cond_items = [str(i) for i in self.cond_items] if colnames: cond_items = [str(colnames[i]) for i in self.cond_items] cond_items = ", ".join(cond_items) print('\r%d itemset(s) from tree conditioned on items (%s)' % (count, cond_items), end="\n")#构建节点class FPNode(object): def __init__(self, item, count=0, parent=None): self.item = item self.count = count self.parent = parent self.children = collections.defaultdict(FPNode) if parent is not None: parent.children[item] = self # 返回自上而下的项目的顺序，从自身（但不包括）到根节点。 def itempath_from_root(self): path = [] if self.item is None: return path # append() 方法用于在列表末尾添加新的对象。 node = self.parent while node.item is not None: path.append(node.item) node = node.parent # reverse() 函数用于反向列表中元素。 path.reverse() return path association_rules.py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169from itertools import combinationsimport numpy as npimport pandas as pddef association_rules(df, metric="confidence", min_threshold=0.8, support_only=False): """生成包括规则“得分”，“可信度”和“提升度”的关联规则的DataFrame 参数 ----------- df : pandas DataFrame ['support', 'itemsets']构成的频繁项集 metric : string (default: 'confidence') 评估规则是否有意义的度量。 **自动设置为 'support' if `support_only=True`.** Otherwise, supported metrics are 'support', 'confidence', 'lift', 'leverage', and 'conviction' These metrics are computed as follows: - support(A-&gt;C) = support(A+C) [aka 'support'], range: [0, 1]\n - confidence(A-&gt;C) = support(A+C) / support(A), range: [0, 1]\n - lift(A-&gt;C) = confidence(A-&gt;C) / support(C), range: [0, inf]\n - leverage(A-&gt;C) = support(A-&gt;C) - support(A)*support(C), range: [-1, 1]\n - conviction = [1 - support(C)] / [1 - confidence(A-&gt;C)], range: [0, inf]\n min_threshold : float (default: 0.8) 评估指标的最低阈值，以确定是否有候选规则感兴趣。 support_only : bool (default: False) 仅计算支持度，并用NaN填充其他指标列。 这在以下情况下有用： a) 输入的DataFrame不完整，例如不包含所有规则前提的支持值和结果 b) 您只是想加快计算速度，因为您不需要其他指标。 Returns ---------- pandas DataFrame with columns "antecedents" and "consequents" that store itemsets, plus the scoring metric columns: "antecedent support", "consequent support", "support", "confidence", "lift", "leverage", "conviction" of all rules for which metric(rule) &gt;= min_threshold. Each entry in the "antecedents" and "consequents" columns are of type `frozenset`, which is a Python built-in type that behaves similarly to sets except that it is immutable """ # check for mandatory columns if not all(col in df.columns for col in ["support", "itemsets"]): raise ValueError("Dataframe needs to contain the\ columns 'support' and 'itemsets'") def conviction_helper(sAC, sA, sC): confidence = sAC/sA conviction = np.empty(confidence.shape, dtype=float) if not len(conviction.shape): conviction = conviction[np.newaxis] confidence = confidence[np.newaxis] sAC = sAC[np.newaxis] sA = sA[np.newaxis] sC = sC[np.newaxis] conviction[:] = np.inf conviction[confidence &lt; 1.] = ((1. - sC[confidence &lt; 1.]) / (1. - confidence[confidence &lt; 1.])) return conviction # metrics for association rules metric_dict = &#123; "antecedent support": lambda _, sA, __: sA, "consequent support": lambda _, __, sC: sC, "support": lambda sAC, _, __: sAC, "confidence": lambda sAC, sA, _: sAC/sA, "lift": lambda sAC, sA, sC: metric_dict["confidence"](sAC, sA, sC)/sC, "leverage": lambda sAC, sA, sC: metric_dict["support"]( sAC, sA, sC) - sA*sC, "conviction": lambda sAC, sA, sC: conviction_helper(sAC, sA, sC) &#125; columns_ordered = ["antecedent support", "consequent support", "support", "confidence", "lift", "leverage", "conviction"] # check for metric compliance if support_only: metric = 'support' else: if metric not in metric_dict.keys(): raise ValueError("Metric must be 'confidence' or 'lift', got '&#123;&#125;'" .format(metric)) # get dict of &#123;frequent itemset&#125; -&gt; support keys = df['itemsets'].values values = df['support'].values frozenset_vect = np.vectorize(lambda x: frozenset(x)) frequent_items_dict = dict(zip(frozenset_vect(keys), values)) # prepare buckets to collect frequent rules rule_antecedents = [] rule_consequents = [] rule_supports = [] # iterate over all frequent itemsets for k in frequent_items_dict.keys(): sAC = frequent_items_dict[k] # to find all possible combinations for idx in range(len(k)-1, 0, -1): # of antecedent and consequent for c in combinations(k, r=idx): antecedent = frozenset(c) consequent = k.difference(antecedent) if support_only: # support doesn't need these, # hence, placeholders should suffice sA = None sC = None else: try: sA = frequent_items_dict[antecedent] sC = frequent_items_dict[consequent] except KeyError as e: s = (str(e) + 'You are likely getting this error' ' because the DataFrame is missing ' ' antecedent and/or consequent ' ' information.' ' You can try using the ' ' `support_only=True` option') raise KeyError(s) # check for the threshold score = metric_dict[metric](sAC, sA, sC) if score &gt;= min_threshold: rule_antecedents.append(antecedent) rule_consequents.append(consequent) rule_supports.append([sAC, sA, sC]) # check if frequent rule was generated if not rule_supports: return pd.DataFrame( columns=["antecedents", "consequents"] + columns_ordered) else: # generate metrics rule_supports = np.array(rule_supports).T.astype(float) df_res = pd.DataFrame( data=list(zip(rule_antecedents, rule_consequents)), columns=["antecedents", "consequents"]) if support_only: sAC = rule_supports[0] for m in columns_ordered: df_res[m] = np.nan df_res['support'] = sAC else: sAC = rule_supports[0] sA = rule_supports[1] sC = rule_supports[2] for m in columns_ordered: df_res[m] = metric_dict[m](sAC, sA, sC) return df_res]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
        <tag>FP-growth</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F12%2F31%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于博客的基础美化]]></title>
    <url>%2F2019%2F06%2F20%2F%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%9F%BA%E7%A1%80%E7%BE%8E%E5%8C%96%2F</url>
    <content type="text"><![CDATA[关于博客的基础美化(next) 一、基础配置 二、主题配置 1.安装主题 2.遇到的问题 1. 创建“分类”页面 2. 创建“标签”页面 3. 文章首页摘要添加图片 4.TOC 5.网页背景图的设置 6. 添加音乐播放器 关于博客的基础美化(next)一、基础配置博客根目录下_config.yml文件包含博客全局的基础配置 12345678# Sitetitle: 王秋霖的博客 # 站点标题subtitle: # 站点副标题description: 这是一个博客 # 站点描述keywords: #关键字author: 王秋霖 # 作者language: zh-CN # 语言，官方不再支持zh-Hans，而是统一采用了zh-CNtimezone: # 时区——默认是本地时区 注意修改每项参数时:后都要加空格否则会报错。 下面可以配置主题和部署的站点： 12345678910## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: hexo-theme-next-master# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/286259788/286259788-github.io.git branch: master 二、主题配置hexo本身提供了很多现成的主题，试过几个选用了比较有名的next主题，经过频繁试错大致完成，记录一些踩到的坑。 1.安装主题从git上下载到的压缩包解压到themes文件夹下，修改上图中的主题。hexo s 后发现初始的next主题很简单，需要逐渐添加功能。打开\themes\hexo-theme-next-master\下的_config.yml(主题配置文件)，其中有非常多的功能，逐行理解。注意修改配置文件或博文并保存后刷新 http://localhost:4000 即可看到更改，无需停止重启。 2.遇到的问题修改过程中如果出错命令行中会有提示，所以要改几步看一下命令行，下面写一下印象比较深的坑。 1. 创建“分类”页面 新建分类页面 1hexo new page categories 给分类页面添加类型 我们在source文件夹中的categories文件夹下找到index.md文件，并在它的头部加上type属性。 12345---title: 文章分类date: 2017-05-27 13:47:40type: &quot;categories&quot; #这部分是新添加的--- 给模板添加分类属性 现在我们打开scarffolds文件夹里的post.md文件，给它的头部加上categories:，这样我们创建的所有新的文章都会自带这个属性，我们只需要往里填分类，就可以自动在网站上形成分类了。 1234title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories:tags: 给文章添加分类 现在我们可以找到一篇文章，然后尝试给它添加分类 123456title: 尝试做博客+Markdown语法简记date: 2019-06-19 22:09:31categories: 记录tags: - md- 博客 2. 创建“标签”页面创建”标签”页的方式和创建“分类”一样。 新建“标签”页面 1hexo new page tags 给标签页面添加类型 我们在source文件夹中的tags文件夹下找到index.md文件，并在它的头部加上type属性。 123title: tagsdate: 2018-08-06 22:48:29type: &quot;tags&quot; #新添加的内容 给文章添加标签 有两种写法都可以，第一种是类似数组的写法，把标签放在中括号[]里，用英文逗号隔开 1234title: 尝试做博客+Markdown语法简记date: 2019-06-19 22:09:31categories: 记录tags: [md,博客] 第二种写法是用-短划线列出来 123456title: 尝试做博客+Markdown语法简记date: 2019-06-19 22:09:31categories: 记录tags: - md- 博客 这两项主要总忘在参数前加括号导致报错。 3. 文章首页摘要添加图片只要在博文的头中添加phtos属性即可，如下图 12345678title: 尝试做博客+Markdown语法简记date: 2019-06-19 22:09:31categories: 记录tags: - md- 博客photos: - &quot;https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=567142806,2737706881&amp;fm=11&amp;gp=0.jpg&quot; 4.TOC文章支持Toc，修改下面enable为true即可。 1234567891011toc: enable: true # Automatically add list number to toc. number: false # If true, all words will placed on next lines if header width longer then sidebar width. wrap: false # If true, all level of TOC in a post will be displayed, rather than the activated part of it. expand_all: false # Maximum heading depth of generated toc. You can set it in one post through `toc_max_depth` in Front-matter. max_depth: 6 注意要使用&lt;!-- toc --&gt; 这种格式，不支持[TOC] 5.网页背景图的设置修改themes\next\source\css\ _custom\custom.styl文件，这个是Next留给用户个性化定制样式的文件，添加以下代码：这里采用了一个图网提供的api接口。 1234567891011121314151617181920// Custom styles.// 添加背景图片body &#123; background: url(https://source.unsplash.com/random/1600x900?wallpapers); background-size: cover; background-repeat: no-repeat; background-attachment: fixed; background-position: 50% 50%;&#125;// 修改主体透明度.main-inner &#123; background: #fff; opacity: 0.8;&#125;// 修改菜单栏透明度.header-inner &#123; opacity: 1;&#125; 这里修改菜单栏透明度不为 1 会使搜索栏变得非常透明，目前未解决，只能设成 1 。 6. 添加音乐播放器使用网易云音乐网页版“生成外链播放器”，复制iframe插件的代码，分成两部分，插入到/layout/_macro/ sidebar.swig文件中,可以选择自己想要的位置和长宽等属性，这里选择在侧边栏social函数后也就是社交网站图标下方。 12345&#123;% if theme.background_music %&#125; &lt;div&gt; &lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="210" height="110" src="&#123;&#123; theme.background_music &#125;&#125;"&gt;&lt;/iframe&gt; &lt;/div&gt;&#123;% endif %&#125; 在主题配置文件中添加background_music,位置随意。把歌单链接放在这，以后可以在这简单的改歌单的地址。 1background_music: //music.163.com/outchain/player?type=0&amp;id=768509742&amp;auto=1&amp;height=66 更新：网易云歌单现在只能完全免费的歌曲才能生成外链，审查元素那招已经无效。]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[尝试做博客+Markdown语法简记]]></title>
    <url>%2F2019%2F06%2F19%2F%E5%B0%9D%E8%AF%95%E5%81%9A%E5%8D%9A%E5%AE%A2%2BMarkdown%E8%AF%AD%E6%B3%95%E7%AE%80%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[基于hexo搭建静态博客​ 今天参考codesheep在b站的视频手把手教你从0开始搭建自己的个人博客尝试搭建博客，安装hexo、node.js和Typora等工具。接触了新的文本编辑方式markdown格式。视频讲的很清楚，这里不再赘述。中间遇到的卡顿多是命令没输对。最后deploy到github上的时候把 . 写成了 - 导致无法访问查了很久。刚搭建好的博客很简陋，以后慢慢美化，有一个问题是”引用“的格式在默认主题下会显示错误，更换主题后显示正常。 ​ 学习了github的一个新用法，这不只是一个代码存储仓库，功能很强大。 Markdown语法写博客等需要用markdown(后缀.md)格式。不用考虑排版问题，可以专注于文章内容本身，跨平台兼容性良好。由于图片等格式都是以文字代码方式保存，文件体积小巧，之前看到过没有深入了解可惜了。下面测试一些基础用法，搬运自简书的一篇文章 一、标题在想要设置为标题的文字前面加#来表示一个#是一级标题，二个#是二级标题，以此类推。支持六级标题。 注：标准语法一般在#后跟个空格再写文字，貌似简书不加空格也行。 示例： 123456# 这是一级标题## 这是二级标题### 这是三级标题#### 这是四级标题##### 这是五级标题###### 这是六级标题 效果如下： 这是一级标题这是二级标题这是三级标题这是四级标题这是五级标题这是六级标题 二、字体语法： 1234要加粗的文字左右分别用两个*号包起来要倾斜的文字左右分别用一个*号包起来要倾斜和加粗的文字左右分别用三个*号包起来要加删除线的文字左右分别用两个~~号包起来 示例： 1234**这是加粗的文字***这是倾斜的文字*`***这是斜体加粗的文字***~~这是加删除线的文字~~ 效果如下： 这是加粗的文字这是倾斜的文字这是斜体加粗的文字这是加删除线的文字 三、引用在引用的文字前加&gt;即可。引用也可以嵌套，如加两个&gt;&gt;三个&gt;&gt;&gt;n个…貌似可以一直加下去，但没神马卵用 示例： 123&gt;这是引用的内容&gt;&gt;这是引用的内容&gt;&gt;&gt;这是引用的内容 效果如下： 这是引用的内容 这是引用的内容 这是引用的内容 四、分割线三个或者三个以上的 - 或者 * 都可以。 示例： 1234-------******** 效果如下：可以看到，显示效果是一样的。 五、图片语法： 1234![图片alt](图片地址 &apos;&apos;图片title&apos;&apos;)图片alt就是显示在图片下面的文字，相当于对图片内容的解释。图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加 示例： 1![cytus2✖Miku](https://ss0.bdstatic.com/94oJfD_bAAcT8t7mm9GUKT-xh_/timg?image&amp;quality=100&amp;size=b4000_4000&amp;sec=1560956929&amp;di=75fa9c14fa24a75ba0c53d786073070d&amp;src=http://wx3.sinaimg.cn/orj360/8900556dly1g220fr8y9fj21fu11q4qq.jpg) 效果如下： 上传本地图片直接点击导航栏的图片标志，选择图片即可 六、超链接语法： 12[超链接名](超链接地址 &quot;超链接title&quot;)title可加可不加 示例： 12[简书](http://jianshu.com)[百度](http://baidu.com) 效果如下： 简书百度 注：Markdown本身语法不支持链接在新页面中打开，貌似简书做了处理，是可以的。别的平台可能就不行了，如果想要在新页面中打开的话可以用html语言的a标签代替。 1234&lt;a href=&quot;超链接地址&quot; target=&quot;_blank&quot;&gt;超链接名&lt;/a&gt;示例&lt;a href=&quot;https://www.jianshu.com/u/1f5ac0cf6a8b&quot; target=&quot;_blank&quot;&gt;简书&lt;/a&gt; 七、列表无序列表语法：无序列表用 - + * 任何一种都可以 12345- 列表内容+ 列表内容* 列表内容注意：- + * 跟内容之间都要有一个空格 效果如下： 列表内容 列表内容 列表内容 有序列表语法：数字加点 123451. 列表内容2. 列表内容3. 列表内容注意：序号跟内容之间要有空格 效果如下： 列表内容 列表内容 列表内容 列表嵌套上一级和下一级之间敲三个空格即可 一级无序列表内容 二级无序列表内容 二级无序列表内容 二级无序列表内容 一级无序列表内容 二级有序列表内容 二级有序列表内容 二级有序列表内容 一级有序列表内容 二级无序列表内容 二级无序列表内容 二级无序列表内容 一级有序列表内容 二级有序列表内容 二级有序列表内容 二级有序列表内容 八、表格语法： 1234567891011表头|表头|表头---|:--:|---:内容|内容|内容内容|内容|内容第二行分割表头和内容。- 有一个就行，为了对齐，多加了几个文字默认居左-两边加：表示文字居中-右边加：表示文字居右注：原生的语法两边都要用 | 包起来。此处省略 示例： 12345姓名|技能|排行--|:--:|--:刘备|哭|大哥关羽|打|二哥张飞|骂|三弟 效果如下： 姓名 技能 排行 刘备 哭 大哥 关羽 打 二哥 张飞 骂 三弟 九、代码语法： 单行代码：代码之间分别用一个反引号包起来 1`代码内容` 代码块：代码之间分别用三个反引号包起来，且两边的反引号单独占一行 12345(```) 代码... 代码... 代码...(```) 注：为了防止转译，前后三个反引号处加了小括号，实际是没有的。这里只是用来演示，实际中去掉两边小括号即可。 示例： 单行代码 1`create database hero;` 代码块 123456(```) function fun()&#123; echo &quot;这是一句非常牛逼的代码&quot;; &#125; fun();(```) 效果如下： 单行代码 1create database hero; 代码块 1234function fun()&#123; echo &quot;这是一句非常牛逼的代码&quot;;&#125;fun(); 十、流程图123456789​```flowst=&gt;start: 开始op=&gt;operation: My Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op&amp; 123456789101112效果如下：```flowst=&gt;start: 开始op=&gt;operation: My Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op&amp; 作者：高鸿祥 链接：&lt;https://www.jianshu.com/p/191d1e21f7ed&gt; 来源：简书]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>md</tag>
      </tags>
  </entry>
</search>
